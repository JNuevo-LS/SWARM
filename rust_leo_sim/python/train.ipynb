{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import zstandard as zstd\n",
    "import datetime\n",
    "import io\n",
    "import satkit\n",
    "from satkit import satstate, time\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "satkit.utils.update_datafiles()\n",
    "\n",
    "def read_zst(filepath:str):\n",
    "    lines = []\n",
    "    with open(filepath, \"rb\") as file:\n",
    "        decompressor = zstd.ZstdDecompressor()\n",
    "        reader = decompressor.stream_reader(file)\n",
    "        text_stream = io.TextIOWrapper(reader, encoding=\"utf-8\")\n",
    "\n",
    "        for line in text_stream:\n",
    "            lines.append(line.rstrip(\"\\n\"))\n",
    "    return lines\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self, line:str):\n",
    "        orbital_data = line.split(\",\")\n",
    "\n",
    "        dt_time = datetime.datetime.fromtimestamp(float(orbital_data[0].rstrip())) #to be optimized\n",
    "        self.time = time.from_datetime(dt_time)\n",
    "        self.pos_x = float(orbital_data[1])\n",
    "        self.pos_y = float(orbital_data[2])\n",
    "        self.pos_z = float(orbital_data[3])\n",
    "        self.vel_x = float(orbital_data[4])\n",
    "        self.vel_y = float(orbital_data[5])\n",
    "        self.vel_z = float(orbital_data[6])\n",
    "    \n",
    "    def get_position_vector(self):\n",
    "        \"\"\"\n",
    "        Returns the position vector as a numpy array\n",
    "        \"\"\"\n",
    "        return np.array([self.pos_x, self.pos_y, self.pos_z])\n",
    "    \n",
    "    def get_velocity_vector(self):\n",
    "        \"\"\"\n",
    "        Returns the velocity vector as a numpy array\n",
    "        \"\"\"\n",
    "        return np.array([self.vel_x, self.vel_y, self.vel_z])\n",
    "    \n",
    "    def get_velocity_magnitude(self):\n",
    "        \"\"\"\n",
    "        Returns magnitude of the velocity vector, in km/s\n",
    "        \"\"\"\n",
    "        return math.sqrt(self.vel_x**2 + self.vel_y**2 + self.vel_z**2)/1000\n",
    "\n",
    "    def create_SatState(self):\n",
    "        \"\"\"\n",
    "        Returns a SatState object from satkit with the current object's data\n",
    "        \"\"\"\n",
    "        return satstate(self.time, self.get_position_vector(), self.get_velocity_vector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dsgp4\n",
    "from customTLE import CustomTLE as TLE\n",
    "#we use our own, leaner version of TLE\n",
    "\n",
    "def read_blocks(file_lines):\n",
    "    num_tles = int(len(file_lines)/5003) # 2 lines for TLE, 5001 lines for satstates\n",
    "    i = 0\n",
    "    tle_arr = []\n",
    "    state_arr = []\n",
    "    for j in range(num_tles):\n",
    "        upper_end = i + 5003\n",
    "        tle = TLE([file_lines[i].rstrip(), file_lines[i+1].rstrip()])\n",
    "        tle_arr.append(tle)\n",
    "        i+=2\n",
    "        while i < upper_end:\n",
    "            state_arr.append(State(file_lines[i]))\n",
    "            i+=1\n",
    "    return (tle_arr, state_arr)\n",
    "\n",
    "def compute_tsinces(epoch:float, states: List[State]):\n",
    "    \"\"\"\n",
    "    Epoch: Unix timestamp of TLE\n",
    "    States: List of State objects representing satellite states in time\n",
    "    \"\"\"\n",
    "    tsinces = []\n",
    "\n",
    "    for state in states:\n",
    "        tsince = (state.time - epoch) / 60\n",
    "        tsinces.append(tsince)\n",
    "\n",
    "    return tsinces\n",
    "\n",
    "def batch_list(input_list: List[TLE], batch_size: int = 32):\n",
    "    return [input_list[i:i+batch_size] for i in range(0, len(input_list), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = \"/mnt/IronWolfPro8TB/SWARM/data/output/raw/val/integration_12.txt.zst\"\n",
    "\n",
    "lines = read_zst(FILEPATH)\n",
    "states = []\n",
    "\n",
    "num_lines = len(lines)\n",
    "(tle_arr, state_arr) = read_blocks(lines)\n",
    "print(len(tle_arr))\n",
    "print(len(state_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import customMLDSGP4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_tle = tle_arr[0]\n",
    "print(\"init_tle:\")\n",
    "print(init_tle)\n",
    "\n",
    "model = customMLDSGP4.mldsgp4()\n",
    "\n",
    "customMLDSGP4.initialize_tle(init_tle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "from torch.amp import GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "\n",
    "#we will use a SmoothL1 Criterion, which combines MSE and MAE in order to be robust to outliers and get smooth gradients\n",
    "class SmoothL1Loss():\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def forward(self, predicted, target):\n",
    "        return\n",
    "\n",
    "def train_mldsgp4(model: customMLDSGP4, tles_batch: List[TLE], tsinces: List[float], targets: List[State], density = 1, epochs = 100, batch_size = 32):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for i, tle in enumerate(tles_batch):\n",
    "        tle = tle.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tle_expanded = [tle] * density\n",
    "        time_steps = torch.linspace(0, tsinces[i], density, device=device)\n",
    "\n",
    "        with autocast():\n",
    "            ouput_segment_states = model(tle_expanded, time_steps)\n",
    "            loss = criterion(output_segment_states, targets[i].to(device))\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = (total_loss / len(tles_batch))\n",
    "    return (model, avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.SmoothL1Loss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 0.001, weight_decay = 0.05)\n",
    "scheduler = ReduceLROnPlateau(optimizer)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_dataset.dataset import CustomDataset\n",
    "\n",
    "TRAIN_PATH = \"/mnt/IronWolfPro8TB/SWARM/data/output/raw/train\"\n",
    "TEST_PATH = \"/mnt/IronWolfPro8TB/SWARM/data/output/raw/test\"\n",
    "VAL_PATH = \"/mnt/IronWolfPro8TB/SWARM/data/output/raw/val\"\n",
    "\n",
    "train_satellites = CustomDataset(folder = TRAIN_PATH)\n",
    "test_satellites = CustomDataset(folder = TEST_PATH)\n",
    "val_satellites = CustomDataset(folder = VAL_PATH)\n",
    "\n",
    "print(train_satellites[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWARM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
